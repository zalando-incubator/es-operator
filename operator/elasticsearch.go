package operator

import (
	"context"
	"encoding/json"
	"fmt"
	"net/url"
	"sync"
	"time"

	"k8s.io/apimachinery/pkg/runtime"

	log "github.com/sirupsen/logrus"
	zv1 "github.com/zalando-incubator/es-operator/pkg/apis/zalando.org/v1"
	"github.com/zalando-incubator/es-operator/pkg/clientset"
	appsv1 "k8s.io/api/apps/v1"
	v1 "k8s.io/api/core/v1"
	pv1 "k8s.io/api/policy/v1"
	"k8s.io/apimachinery/pkg/api/errors"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/fields"
	"k8s.io/apimachinery/pkg/labels"
	"k8s.io/apimachinery/pkg/types"
	"k8s.io/apimachinery/pkg/util/intstr"
	"k8s.io/apimachinery/pkg/util/wait"
	kubeinformers "k8s.io/client-go/informers"
	informersv1 "k8s.io/client-go/informers/core/v1"
	"k8s.io/client-go/tools/cache"
	kube_record "k8s.io/client-go/tools/record"
)

const (
	esDataSetLabelKey                       = "es-operator-dataset"
	esOperatorAnnotationKey                 = "es-operator.zalando.org/operator"
	esScalingOperationKey                   = "es-operator.zalando.org/current-scaling-operation"
	esMigrationCompleteAnnotationKey        = "es-operator.zalando.org/pod-label-migration-complete"
	defaultElasticsearchDataSetEndpointPort = 9200
)

type ElasticsearchOperator struct {
	logger                *log.Entry
	kube                  *clientset.Clientset
	podInformer           informersv1.PodInformer
	nodeInformer          informersv1.NodeInformer
	interval              time.Duration
	autoscalerInterval    time.Duration
	metricsInterval       time.Duration
	priorityNodeSelectors labels.Set
	operatorID            string
	namespace             string
	clusterDNSZone        string
	elasticsearchEndpoint *url.URL
	skipPodLabelMigration bool
	operating             map[types.UID]operatingEntry
	sync.Mutex
	recorder kube_record.EventRecorder
}

type operatingEntry struct {
	cancel context.CancelFunc
	doneCh <-chan struct{}
	logger *log.Entry
}

// DrainingConfig specifies the configuration settings for the behavior of draining Elasticsearch nodes.
type DrainingConfig struct {
	MaxRetries      int
	MinimumWaitTime time.Duration
	MaximumWaitTime time.Duration
}

// NewElasticsearchOperator initializes a new ElasticsearchDataSet operator instance.
func NewElasticsearchOperator(
	client *clientset.Clientset,
	priorityNodeSelectors map[string]string,
	interval,
	autoscalerInterval time.Duration,
	operatorID,
	namespace,
	clusterDNSZone string,
	elasticsearchEndpoint *url.URL,
	skipPodLabelMigration bool,
) *ElasticsearchOperator {

	return &ElasticsearchOperator{
		logger: log.WithFields(
			log.Fields{
				"operator": "elasticsearch",
			},
		),
		kube:                  client,
		interval:              interval,
		autoscalerInterval:    autoscalerInterval,
		metricsInterval:       60 * time.Second,
		priorityNodeSelectors: labels.Set(priorityNodeSelectors),
		operatorID:            operatorID,
		namespace:             namespace,
		clusterDNSZone:        clusterDNSZone,
		elasticsearchEndpoint: elasticsearchEndpoint,
		skipPodLabelMigration: skipPodLabelMigration,
		operating:             make(map[types.UID]operatingEntry),
		recorder:              createEventRecorder(client),
	}
}

/*
setupInformers sets up informers for pods and nodes, and runs a migration to ensure all EDS pods are labeled.
*/
func (o *ElasticsearchOperator) setupInformers(ctx context.Context) error {
	informerFactory := kubeinformers.NewSharedInformerFactoryWithOptions(o.kube, 0, kubeinformers.WithNamespace(o.namespace))
	podInformer := informerFactory.Core().V1().Pods()
	nodeInformer := informerFactory.Core().V1().Nodes()

	informers := []cache.SharedIndexInformer{
		podInformer.Informer(),
		nodeInformer.Informer(),
	}

	for _, informer := range informers {
		// Add default resource event handlers to properly initialize informer.
		_, err := informer.AddEventHandler(
			cache.ResourceEventHandlerFuncs{
				AddFunc: func(obj interface{}) {},
			},
		)
		if err != nil {
			return fmt.Errorf("failed to add event handler: %v", err)
		}
	}

	informerFactory.Start(ctx.Done())

	for _, informer := range informers {
		// wait for the local cache to be populated.
		err := wait.PollUntilContextTimeout(ctx, time.Second, 60*time.Second, true, func(_ context.Context) (bool, error) {
			return informer.HasSynced() == true, nil
		})
		if err != nil {
			return fmt.Errorf("failed to sync cache for informer: %v", err)
		}
	}

	o.podInformer = podInformer
	o.nodeInformer = nodeInformer

	// MIGRATION: Patch all existing EDS pods to ensure they have the es-operator-dataset label
	if !o.skipPodLabelMigration {
		if err := o.migrateLabelEDSRelatedPods(ctx); err != nil {
			o.logger.WithError(err).Warn("Failed to migrate EDS pod labels")
		}
	} else {
		o.logger.Info("Skipping pod-label migration as requested")
	}

	return nil
}

// migrateLabelEDSRelatedPods ensures all pods for each EDS have the es-operator-dataset label.
// This is a one-time migration for clusters upgrading to label-based pod selection.
// Uses annotations to track migration completion and avoid running migration repeatedly.
func (o *ElasticsearchOperator) migrateLabelEDSRelatedPods(ctx context.Context) error {
	logger := o.logger.WithField("component", "migration")
	logger.Info("Starting EDS pod label migration")

	var migratedCount, skippedCount, errorCount int

	edss, err := o.kube.ZalandoV1().ElasticsearchDataSets(o.namespace).List(ctx, metav1.ListOptions{})
	if err != nil {
		return fmt.Errorf("failed to list EDSs for migration: %w", err)
	}

	for _, eds := range edss.Items {
		edsLogger := logger.WithField("eds", eds.Name)

		// Check if migration has already been completed for this EDS
		if eds.Annotations != nil {
			if _, exists := eds.Annotations[esMigrationCompleteAnnotationKey]; exists {
				edsLogger.Debug("Migration already completed for EDS, skipping")
				skippedCount++
				continue
			}
		}

		edsLogger.Info("Starting migration for EDS")

		// Check if this EDS needs migration by looking for proper owner references
		migrated, err := o.migratePodLabelsForEDS(ctx, &eds)
		if err != nil {
			edsLogger.WithError(err).Warn("Failed to migrate pod labels for EDS")
			errorCount++
			continue
		}

		if migrated {
			migratedCount++
		} else {
			skippedCount++
		}

		// Mark migration as complete for this EDS
		if err := o.markMigrationComplete(ctx, &eds); err != nil {
			edsLogger.WithError(err).Warn("Failed to mark migration as complete")
			errorCount++
		} else {
			edsLogger.Debug("Marked migration as complete for EDS")
		}
	}

	if errorCount > 0 {
		logger.Warnf("Migration completed with errors: migrated=%d, skipped=%d, errors=%d",
			migratedCount, skippedCount, errorCount)
	} else {
		logger.Infof("Migration completed successfully: migrated=%d, skipped=%d",
			migratedCount, skippedCount)
	}

	return nil
}

// migratePodLabelsForEDS migrates pod labels for a specific EDS using proper owner reference validation
func (o *ElasticsearchOperator) migratePodLabelsForEDS(ctx context.Context, eds *zv1.ElasticsearchDataSet) (bool, error) {
	logger := o.logger.WithField("eds", eds.Name)

	// Get all pods in the namespace using informer cache instead of direct API call
	pods, err := o.podInformer.Lister().Pods(o.namespace).List(labels.Everything())
	if err != nil {
		return false, fmt.Errorf("failed to list pods: %w", err)
	}

	var podsPatched int
	for _, pod := range pods {
		// Skip pods that already have the label
		if pod.Labels != nil {
			if _, ok := pod.Labels[esDataSetLabelKey]; ok {
				continue
			}
		}

		// Check if this pod is owned by the EDS using proper owner reference validation
		if o.isPodOwnedByEDS(ctx, pod, eds) {
			// Patch the pod to add the label
			patch := []byte(fmt.Sprintf(`{"metadata":{"labels":{"%s":"%s"}}}`, esDataSetLabelKey, eds.Name))
			_, err := o.kube.CoreV1().Pods(o.namespace).Patch(ctx, pod.Name, types.StrategicMergePatchType, patch, metav1.PatchOptions{})
			if err != nil {
				logger.WithError(err).Warnf("Failed to patch pod %s", pod.Name)
				return false, err
			} else {
				logger.Infof("Patched pod %s to add label %s=%s", pod.Name, esDataSetLabelKey, eds.Name)
				podsPatched++
			}
		}
	}

	return podsPatched > 0, nil
}

// isPodOwnedByEDS checks if a pod is owned by the given EDS through proper owner reference validation
func (o *ElasticsearchOperator) isPodOwnedByEDS(ctx context.Context, pod *v1.Pod, eds *zv1.ElasticsearchDataSet) bool {
	for _, ref := range pod.OwnerReferences {
		if ref.Kind == "StatefulSet" {
			// Check if StatefulSet is owned by this EDS
			sts, err := o.kube.AppsV1().StatefulSets(pod.Namespace).Get(ctx, ref.Name, metav1.GetOptions{})
			if err == nil {
				for _, stsRef := range sts.OwnerReferences {
					if stsRef.UID == eds.UID {
						return true
					}
				}
			}
		}
	}
	return false
}

// markMigrationComplete marks the migration as complete by adding an annotation to the EDS
func (o *ElasticsearchOperator) markMigrationComplete(ctx context.Context, eds *zv1.ElasticsearchDataSet) error {
	// Create the patch to add the migration complete annotation
	patch := []byte(fmt.Sprintf(`{"metadata":{"annotations":{"%s":"true"}}}`, esMigrationCompleteAnnotationKey))
	_, err := o.kube.ZalandoV1().ElasticsearchDataSets(o.namespace).Patch(ctx, eds.Name, types.StrategicMergePatchType, patch, metav1.PatchOptions{})
	return err
}

// Run runs the main loop of the operator.
func (o *ElasticsearchOperator) Run(ctx context.Context) error {
	err := o.setupInformers(ctx)
	if err != nil {
		return err
	}

	go o.collectMetrics(ctx)
	go o.runAutoscaler(ctx)

	// run EDS watcher
	err = o.runWatch(ctx)
	if err != nil {
		return err
	}
	<-ctx.Done()
	o.logger.Info("Terminating main operator loop.")
	return nil
}

// Run setups up a shared informer for listing and watching changes to pods and
// starts listening for events.
func (o *ElasticsearchOperator) runWatch(ctx context.Context) error {
	informer := cache.NewSharedIndexInformer(
		cache.NewListWatchFromClient(
			o.kube.ZalandoV1().RESTClient(),
			"elasticsearchdatasets",
			o.namespace, fields.Everything(),
		),
		&zv1.ElasticsearchDataSet{},
		0, // skip resync
		cache.Indexers{},
	)

	_, err := informer.AddEventHandler(cache.ResourceEventHandlerFuncs{
		AddFunc:    o.add,
		UpdateFunc: o.update,
		DeleteFunc: o.del,
	})

	if err != nil {
		return fmt.Errorf("Failed to add event handler: %w", err)
	}

	go informer.Run(ctx.Done())

	if !cache.WaitForCacheSync(ctx.Done(), informer.HasSynced) {
		return fmt.Errorf("Timed out waiting for caches to sync")
	}

	log.Info("Synced ElasticsearchDataSet watcher")
	return nil
}

// add is the handler for an EDS getting added.
func (o *ElasticsearchOperator) add(obj interface{}) {
	eds, ok := obj.(*zv1.ElasticsearchDataSet)
	if !ok {
		log.Errorf("Failed to get EDS object")
		return
	}

	err := o.operateEDS(eds, false)
	if err != nil {
		log.Errorf("Add failed, this is bad!: %v", err)
	}
}

// update is the handler for an EDS getting updated.
func (o *ElasticsearchOperator) update(oldObj, newObj interface{}) {
	newEDS, ok := newObj.(*zv1.ElasticsearchDataSet)
	if !ok {
		log.Errorf("Failed to get EDS object")
		return
	}

	err := o.operateEDS(newEDS, false)
	if err != nil {
		log.Errorf("Add failed, this is bad!: %v", err)
	}
}

// del is the handler for an EDS getting deleted.
func (o *ElasticsearchOperator) del(obj interface{}) {
	eds, ok := obj.(*zv1.ElasticsearchDataSet)
	if !ok {
		log.Errorf("Failed to get EDS object")
		return
	}

	err := o.operateEDS(eds, true)
	if err != nil {
		log.Errorf("Add failed, this is bad!: %v", err)
	}
}

// collectMetrics collects metrics for all the managed EDS resources.
// The metrics are stored in the coresponding ElasticsearchMetricSet and used
// by the autoscaler for scaling EDS.
func (o *ElasticsearchOperator) collectMetrics(ctx context.Context) {
	nextCheck := time.Now().Add(-o.metricsInterval)

	for {
		o.logger.Debug("Collecting metrics")
		select {
		case <-time.After(time.Until(nextCheck)):
			nextCheck = time.Now().Add(o.metricsInterval)

			resources, err := o.collectResources(ctx)
			if err != nil {
				o.logger.Error(err)
				continue
			}

			for _, es := range resources {
				if es.ElasticsearchDataSet.Spec.Scaling != nil && es.ElasticsearchDataSet.Spec.Scaling.Enabled {
					metrics := &ElasticsearchMetricsCollector{
						kube:   o.kube,
						logger: log.WithFields(log.Fields{"collector": "metrics"}),
						es:     *es,
					}
					err := metrics.collectMetrics(ctx)
					if err != nil {
						o.logger.Error(err)
						continue
					}
				}
			}
		case <-ctx.Done():
			o.logger.Info("Terminating metrics collector loop.")
			return
		}
	}
}

// runAutoscaler runs the EDS autoscaler which checks at an interval if any
// EDS resources needs to be autoscaled. If autoscaling is needed this will be
// indicated on the EDS with the
// 'es-operator.zalando.org/current-scaling-operation' annotation. The
// annotation indicates the desired scaling which will be reconciled by the
// operator.
func (o *ElasticsearchOperator) runAutoscaler(ctx context.Context) {
	nextCheck := time.Now().Add(-o.autoscalerInterval)

	for {
		o.logger.Debug("Checking autoscaling")
		select {
		case <-time.After(time.Until(nextCheck)):
			nextCheck = time.Now().Add(o.autoscalerInterval)

			resources, err := o.collectResources(ctx)
			if err != nil {
				o.logger.Error(err)
				continue
			}

			for _, es := range resources {
				if es.ElasticsearchDataSet.Spec.Scaling != nil && es.ElasticsearchDataSet.Spec.Scaling.Enabled {
					endpoint := o.getElasticsearchEndpoint(es.ElasticsearchDataSet)
					client := &ESClient{
						Endpoint:             endpoint,
						excludeSystemIndices: es.ElasticsearchDataSet.Spec.ExcludeSystemIndices,
						DrainingConfig:       o.getDrainingConfig(es.ElasticsearchDataSet),
					}

					err := o.scaleEDS(ctx, es.ElasticsearchDataSet, es, client)
					if err != nil {
						o.logger.Error(err)
						continue
					}
				}
			}
		case <-ctx.Done():
			o.logger.Info("Terminating autoscaler loop.")
			return
		}
	}
}

type EDSResource struct {
	eds      *zv1.ElasticsearchDataSet
	kube     *clientset.Clientset
	esClient *ESClient
	recorder kube_record.EventRecorder
}

func (r *EDSResource) Name() string {
	return r.eds.Name
}

func (r *EDSResource) Namespace() string {
	return r.eds.Namespace
}

func (r *EDSResource) APIVersion() string {
	return r.eds.APIVersion
}

func (r *EDSResource) Kind() string {
	return r.eds.Kind
}

func (r *EDSResource) Labels() map[string]string {
	return r.eds.Labels
}

func (r *EDSResource) LabelSelector() map[string]string {
	return map[string]string{esDataSetLabelKey: r.Name()}
}

func (r *EDSResource) Generation() int64 {
	return r.eds.Generation
}

func (r *EDSResource) UID() types.UID {
	return r.eds.UID
}

func (r *EDSResource) Replicas() *int32 {
	return edsReplicas(r.eds)
}

func (r *EDSResource) PodTemplateSpec() *v1.PodTemplateSpec {
	template := r.eds.Spec.Template.DeepCopy()
	return &v1.PodTemplateSpec{
		ObjectMeta: metav1.ObjectMeta{
			Annotations: template.Annotations,
			Labels:      template.Labels,
		},
		Spec: template.Spec,
	}
}

func (r *EDSResource) VolumeClaimTemplates() []v1.PersistentVolumeClaim {
	claims := make([]v1.PersistentVolumeClaim, 0, len(r.eds.Spec.VolumeClaimTemplates))
	for _, template := range r.eds.Spec.VolumeClaimTemplates {
		claims = append(claims, v1.PersistentVolumeClaim{
			ObjectMeta: metav1.ObjectMeta{
				Name:        template.Name,
				Labels:      template.Labels,
				Annotations: template.Annotations,
			},
			Spec: template.Spec,
		})
	}
	return claims
}

func (r *EDSResource) EnsureResources(ctx context.Context) error {
	// ensure PDB
	err := r.ensurePodDisruptionBudget(ctx)
	if err != nil {
		return err
	}

	// ensure service
	err = r.ensureService(ctx)
	if err != nil {
		return err
	}

	return nil
}

func (r *EDSResource) Self() runtime.Object {
	return r.eds
}

// ensurePodDisruptionBudget creates a PodDisruptionBudget for the
// ElasticsearchDataSet if it doesn't already exist.
func (r *EDSResource) ensurePodDisruptionBudget(ctx context.Context) error {
	var pdb *pv1.PodDisruptionBudget
	var err error

	pdb, err = r.kube.PolicyV1().PodDisruptionBudgets(r.eds.Namespace).Get(ctx, r.eds.Name, metav1.GetOptions{})
	if err != nil {
		if !errors.IsNotFound(err) {
			return fmt.Errorf(
				"failed to get PodDisruptionBudget for %s %s/%s: %v",
				r.eds.Kind,
				r.eds.Namespace, r.eds.Name,
				err,
			)
		}
		pdb = nil
	}

	// check if owner
	if pdb != nil && !isOwnedReference(r, pdb.ObjectMeta) {
		return fmt.Errorf(
			"PodDisruptionBudget %s/%s is not owned by the %s %s/%s",
			pdb.Namespace, pdb.Name,
			r.eds.Kind,
			r.eds.Namespace, r.eds.Name,
		)
	}

	matchLabels := r.LabelSelector()

	createPDB := false

	if pdb == nil {
		createPDB = true
		maxUnavailable := intstr.FromInt(0)
		pdb = &pv1.PodDisruptionBudget{
			ObjectMeta: metav1.ObjectMeta{
				Name:      r.eds.Name,
				Namespace: r.eds.Namespace,
				OwnerReferences: []metav1.OwnerReference{
					{
						APIVersion: r.eds.APIVersion,
						Kind:       r.eds.Kind,
						Name:       r.eds.Name,
						UID:        r.eds.UID,
					},
				},
			},
			Spec: pv1.PodDisruptionBudgetSpec{
				MaxUnavailable: &maxUnavailable,
			},
		}
	}

	pdb.Labels = r.eds.Labels
	pdb.Spec.Selector = &metav1.LabelSelector{
		MatchLabels: matchLabels,
	}

	if createPDB {
		var err error
		_, err = r.kube.PolicyV1().PodDisruptionBudgets(pdb.Namespace).Create(ctx, pdb, metav1.CreateOptions{})
		if err != nil {
			return fmt.Errorf(
				"failed to create PodDisruptionBudget for %s %s/%s: %v",
				r.eds.Kind,
				r.eds.Namespace, r.eds.Name,
				err,
			)
		}
		r.recorder.Event(r.eds, v1.EventTypeNormal, "CreatedPDB", fmt.Sprintf(
			"Created PodDisruptionBudget '%s/%s' for %s",
			pdb.Namespace, pdb.Name, r.eds.Kind,
		))
	}

	return nil
}

// ensureService creates a Service for the ElasticsearchDataSet if it doesn't
// already exist.
func (r *EDSResource) ensureService(ctx context.Context) error {
	var svc *v1.Service
	var err error

	svc, err = r.kube.CoreV1().Services(r.eds.Namespace).Get(ctx, r.eds.Name, metav1.GetOptions{})
	if err != nil {
		if !errors.IsNotFound(err) {
			return fmt.Errorf(
				"failed to get Service for %s %s/%s: %v",
				r.eds.Kind,
				r.eds.Namespace, r.eds.Name,
				err,
			)
		}
		svc = nil
	}

	// check if owner
	if svc != nil && !isOwnedReference(r, svc.ObjectMeta) {
		return fmt.Errorf(
			"the Service '%s/%s' is not owned by the %s '%s/%s'",
			svc.Namespace, svc.Name,
			r.eds.Kind,
			r.eds.Namespace, r.eds.Name,
		)
	}

	matchLabels := r.LabelSelector()

	createService := false

	if svc == nil {
		createService = true
		svc = &v1.Service{
			ObjectMeta: metav1.ObjectMeta{
				Name:      r.eds.Name,
				Namespace: r.eds.Namespace,
				OwnerReferences: []metav1.OwnerReference{
					{
						APIVersion: r.eds.APIVersion,
						Kind:       r.eds.Kind,
						Name:       r.eds.Name,
						UID:        r.eds.UID,
					},
				},
			},
			Spec: v1.ServiceSpec{
				Type: v1.ServiceTypeClusterIP,
			},
		}
	}

	svc.Labels = r.eds.Labels
	svc.Spec.Selector = matchLabels
	// TODO: derive port from EDS
	svc.Spec.Ports = []v1.ServicePort{
		{
			Name:       "elasticsearch",
			Protocol:   v1.ProtocolTCP,
			Port:       defaultElasticsearchDataSetEndpointPort,
			TargetPort: intstr.FromInt(defaultElasticsearchDataSetEndpointPort),
		},
	}

	if createService {
		var err error
		_, err = r.kube.CoreV1().Services(svc.Namespace).Create(ctx, svc, metav1.CreateOptions{})
		if err != nil {
			return fmt.Errorf(
				"failed to create Service for %s %s/%s: %v",
				r.eds.Kind,
				r.eds.Namespace, r.eds.Name,
				err,
			)
		}
		r.recorder.Event(r.eds, v1.EventTypeNormal, "CreatedService", fmt.Sprintf(
			"Created Service '%s/%s' for %s",
			svc.Namespace, svc.Name, r.eds.Kind,
		))
	}

	return nil
}

// Drain drains a pod for Elasticsearch data.
func (r *EDSResource) Drain(ctx context.Context, pod *v1.Pod) error {
	if r.eds.Spec.SkipDraining {
		return nil
	}
	return r.esClient.Drain(ctx, pod)
}

// PreScaleDownHook ensures that the IndexReplicas is set as defined in the EDS
// 'scaling-operation' annotation prior to scaling down the internal
// StatefulSet.
func (r *EDSResource) PreScaleDownHook(ctx context.Context) error {
	return r.applyScalingOperation(ctx)
}

// OnStableReplicasHook ensures that the indexReplicas is set as defined in the
// EDS scaling-operation annotation.
func (r *EDSResource) OnStableReplicasHook(ctx context.Context) error {
	err := r.applyScalingOperation(ctx)
	if err != nil {
		return err
	}

	// cleanup state in ES
	return r.esClient.Cleanup(ctx)
}

// UpdateStatus updates the status of the EDS to set the current replicas from
// StatefulSet and updating the observedGeneration.
func (r *EDSResource) UpdateStatus(ctx context.Context, sts *appsv1.StatefulSet) error {
	observedGeneration := int64(0)
	if r.eds.Status.ObservedGeneration != nil {
		observedGeneration = *r.eds.Status.ObservedGeneration
	}

	replicas := int32(0)
	if sts.Spec.Replicas != nil {
		replicas = *sts.Spec.Replicas
	}

	if r.eds.Generation != observedGeneration ||
		r.eds.Status.Replicas != replicas {
		r.eds.Status.Replicas = replicas
		r.eds.Status.ObservedGeneration = &r.eds.Generation
		eds, err := r.kube.ZalandoV1().ElasticsearchDataSets(r.eds.Namespace).UpdateStatus(ctx, r.eds, metav1.UpdateOptions{})
		if err != nil {
			return err
		}

		// set TypeMeta manually because of this bug:
		// https://github.com/kubernetes/client-go/issues/308
		eds.APIVersion = "zalando.org/v1"
		eds.Kind = "ElasticsearchDataSet"
		r.eds = eds
	}

	return nil
}

func (r *EDSResource) applyScalingOperation(ctx context.Context) error {
	operation, err := edsScalingOperation(r.eds)
	if err != nil {
		return err
	}

	if operation != nil && operation.ScalingDirection != NONE {
		err = r.esClient.UpdateIndexSettings(operation.IndexReplicas)
		if err != nil {
			return err
		}
		err = r.removeScalingOperationAnnotation(ctx)
		if err != nil {
			return err
		}
	}
	return nil
}

// removeScalingOperationAnnotation removes the 'scaling-operation' annotation
// from the EDS.
// If the annotation is already gone, this is a no-op.
func (r *EDSResource) removeScalingOperationAnnotation(ctx context.Context) error {
	if _, ok := r.eds.Annotations[esScalingOperationKey]; ok {
		delete(r.eds.Annotations, esScalingOperationKey)
		_, err := r.kube.ZalandoV1().
			ElasticsearchDataSets(r.eds.Namespace).
			Update(ctx, r.eds, metav1.UpdateOptions{})
		if err != nil {
			return fmt.Errorf("failed to remove 'scaling-operating' annotation from EDS: %v", err)
		}
	}
	return nil
}

// calls kubernetes APIs to refresh itself
func (r *EDSResource) Get(ctx context.Context) (StatefulResource, error) {
	newEds, err := r.kube.ZalandoV1().ElasticsearchDataSets(r.eds.Namespace).Get(ctx, r.eds.Name, metav1.GetOptions{})
	if err != nil {
		return nil, err
	}

	// set TypeMeta manually because of this bug:
	// https://github.com/kubernetes/client-go/issues/308
	newEds.Kind = r.Kind()
	newEds.APIVersion = r.APIVersion()
	sr := &EDSResource{
		eds:      newEds,
		kube:     r.kube,
		esClient: r.esClient, // TODO: think about not setting this twice
		recorder: r.recorder,
	}

	return sr, nil
}

func (o *ElasticsearchOperator) operateEDS(eds *zv1.ElasticsearchDataSet, deleted bool) error {
	// set TypeMeta manually because of this bug:
	// https://github.com/kubernetes/client-go/issues/308
	eds.APIVersion = "zalando.org/v1"
	eds.Kind = "ElasticsearchDataSet"

	// insert into operating
	o.Lock()
	defer o.Unlock()

	// restart if already being operated on.
	if entry, ok := o.operating[eds.UID]; ok {
		entry.cancel()
		// wait for previous operation to terminate
		entry.logger.Infof("Waiting for operation to stop")
		<-entry.doneCh
	}

	if deleted {
		return nil
	}

	if !o.hasOwnership(eds) {
		o.logger.Infof("Skipping EDS %s/%s, not owned by the operator", eds.Namespace, eds.Name)
		return nil
	}

	doneCh := make(chan struct{})
	ctx, cancel := context.WithCancel(context.Background())

	logger := log.WithFields(log.Fields{
		"eds":       eds.Name,
		"namespace": eds.Namespace,
	})
	// add to operating table
	o.operating[eds.UID] = operatingEntry{
		cancel: cancel,
		doneCh: doneCh,
		logger: logger,
	}

	endpoint := o.getElasticsearchEndpoint(eds)

	// TODO: abstract this
	client := &ESClient{
		Endpoint:       endpoint,
		DrainingConfig: o.getDrainingConfig(eds),
	}

	operator := &Operator{
		kube:                  o.kube,
		podInformer:           o.podInformer,
		nodeInformer:          o.nodeInformer,
		priorityNodeSelectors: o.priorityNodeSelectors,
		interval:              o.interval,
		logger:                logger,
		recorder:              o.recorder,
	}

	rs := &EDSResource{
		eds:      eds,
		kube:     o.kube,
		esClient: client, // TODO: think about not setting this twice
		recorder: o.recorder,
	}

	go operator.Run(ctx, doneCh, rs)

	return nil
}

// hasOwnership returns true if the operator is the "owner" of the EDS.
// Whether it's owner is determined by the value of the
// 'es-operator.zalando.org/operator' annotation. If the value
// matches the operatorID then it owns it, or if the operatorID is
// "" and there's no annotation set.
func (o *ElasticsearchOperator) hasOwnership(eds *zv1.ElasticsearchDataSet) bool {
	if eds.Annotations != nil {
		if owner, ok := eds.Annotations[esOperatorAnnotationKey]; ok {
			return owner == o.operatorID
		}
	}
	return o.operatorID == ""
}

func (o *ElasticsearchOperator) getElasticsearchEndpoint(eds *zv1.ElasticsearchDataSet) *url.URL {
	if o.elasticsearchEndpoint != nil {
		return o.elasticsearchEndpoint
	}

	// TODO: discover port from EDS
	return &url.URL{
		Scheme: "http",
		Host: fmt.Sprintf(
			"%s.%s.svc.%s:%d",
			eds.Name,
			eds.Namespace,
			o.clusterDNSZone,
			defaultElasticsearchDataSetEndpointPort,
		),
	}
}

// DrainingConfig returns the draining specification which control how should we handle draining nodes.
func (o *ElasticsearchOperator) getDrainingConfig(eds *zv1.ElasticsearchDataSet) *DrainingConfig {
	// Fallback to default configurations if draining configuration is not specified.
	if eds.Spec.Experimental == nil || eds.Spec.Experimental.Draining == nil {
		return &DrainingConfig{
			MaxRetries:      999,
			MinimumWaitTime: 10 * time.Second,
			MaximumWaitTime: 30 * time.Second,
		}
	}
	return &DrainingConfig{
		MaxRetries:      int(eds.Spec.Experimental.Draining.MaxRetries),
		MinimumWaitTime: time.Duration(eds.Spec.Experimental.Draining.MinimumWaitTimeDurationSeconds) * time.Second,
		MaximumWaitTime: time.Duration(eds.Spec.Experimental.Draining.MaximumWaitTimeDurationSeconds) * time.Second,
	}
}

type ESResource struct {
	ElasticsearchDataSet *zv1.ElasticsearchDataSet
	StatefulSet          *appsv1.StatefulSet
	MetricSet            *zv1.ElasticsearchMetricSet
	Pods                 []v1.Pod
}

// Replicas returns the desired replicas for the ElasticsearchDataSet.
func (es *ESResource) Replicas() *int32 {
	return edsReplicas(es.ElasticsearchDataSet)
}

// edsReplicas returns the desired replicas for an ElasticsearchDataSet.
//
// It returns nil to indicate "don't touch".
// When autoscaling is enabled and spec.replicas is not set, it returns
// max(minReplicas, status.replicas, 1) to prevent scale-to-zero.
func edsReplicas(eds *zv1.ElasticsearchDataSet) *int32 {
	scaling := eds.Spec.Scaling
	if scaling == nil || !scaling.Enabled {
		return eds.Spec.Replicas
	}

	if eds.Spec.Replicas != nil {
		return eds.Spec.Replicas
	}

	if scaling.MinReplicas <= 0 {
		return nil
	}

	// Use max(minReplicas, 1) as base to prevent scale-to-zero
	desired := scaling.MinReplicas
	if desired < 1 {
		desired = 1
	}

	if eds.Status.Replicas > 0 && eds.Status.Replicas > desired {
		desired = eds.Status.Replicas
	}
	return &desired
}

/*
collectResources collects all the ElasticsearchDataSet resources and their corresponding StatefulSets, MetricSets, and Pods.
This version uses pod labels to efficiently select only the pods belonging to each EDS.
*/
func (o *ElasticsearchOperator) collectResources(ctx context.Context) (map[types.UID]*ESResource, error) {
	resources := make(map[types.UID]*ESResource)

	edss, err := o.kube.ZalandoV1().ElasticsearchDataSets(o.namespace).List(ctx, metav1.ListOptions{})
	if err != nil {
		return nil, err
	}

	// create a map of ElasticsearchDataSet clusters to later map the matching StatefulSet.
	for _, eds := range edss.Items {
		eds := eds
		if !o.hasOwnership(&eds) {
			continue
		}

		// set TypeMeta manually because of this bug:
		// https://github.com/kubernetes/client-go/issues/308
		eds.APIVersion = "zalando.org/v1"
		eds.Kind = "ElasticsearchDataSet"

		resources[eds.UID] = &ESResource{
			ElasticsearchDataSet: &eds,
		}
	}

	metricSets, err := o.kube.ZalandoV1().ElasticsearchMetricSets(o.namespace).List(ctx, metav1.ListOptions{})
	if err != nil {
		return nil, err
	}

	// Map metricSets to the owning ElasticsearchDataSet resource.
	for _, ms := range metricSets.Items {
		ms := ms
		if uid, ok := getOwnerUID(ms.ObjectMeta); ok {
			if er, ok := resources[uid]; ok {
				er.MetricSet = &ms
			}
		}
	}

	// Use label selectors to efficiently fetch pods for each EDS.
	for _, es := range resources {
		selector := labels.Set(map[string]string{esDataSetLabelKey: es.ElasticsearchDataSet.Name}).AsSelector()
		pods, err := o.podInformer.Lister().Pods(o.namespace).List(selector)
		if err != nil {
			return nil, err
		}
		for _, pod := range pods {
			es.Pods = append(es.Pods, *pod)
		}
	}

	return resources, nil
}

func (o *ElasticsearchOperator) scaleEDS(ctx context.Context, eds *zv1.ElasticsearchDataSet, es *ESResource, client *ESClient) error {
	err := validateScalingSettings(eds.Spec.Scaling)
	if err != nil {
		o.recorder.Event(eds, v1.EventTypeWarning, "ScalingInvalid", fmt.Sprintf(
			"Scaling settings are invalid: %v", err),
		)
		return err
	}

	// first, try to find an existing annotation and return it
	scalingOperation, err := edsScalingOperation(eds)
	if err != nil {
		return err
	}

	// exit early if the scaling operation is already defined
	if scalingOperation != nil && scalingOperation.ScalingDirection != NONE {
		return nil
	}

	// second, calculate a new EDS scaling operation
	scaling := eds.Spec.Scaling

	currentReplicasPtr := edsReplicas(eds)

	// Ensure spec.replicas is initialized to a safe value.
	// If edsReplicas returns nil, we derive a fallback to avoid implicitly
	// scaling to zero.
	if currentReplicasPtr == nil {
		// Default fallback value (minimum 1)
		desired := int32(1)

		if scaling != nil && scaling.MinReplicas > 0 {
			desired = scaling.MinReplicas
			if eds.Status.Replicas > 0 && eds.Status.Replicas > desired {
				desired = eds.Status.Replicas
			}
		} else if eds.Status.Replicas > 0 {
			// Use status as fallback but never below 1
			desired = eds.Status.Replicas
		}

		// Absolute safety check to prevent scale-to-zero
		if desired < 1 {
			log.Infof("EDS %s/%s: Fallback calculation resulted in %d, enforcing minimum of 1", eds.Namespace, eds.Name, desired)
			desired = 1
		}

		currentReplicasPtr = &desired
	}

	currentReplicas := *currentReplicasPtr
	eds.Spec.Replicas = currentReplicasPtr

	if scaling != nil && scaling.Enabled {
		return o.updateScalingOperation(ctx, eds, es, client, currentReplicas)
	}

	return nil
}

func (o *ElasticsearchOperator) updateScalingOperation(ctx context.Context, eds *zv1.ElasticsearchDataSet, es *ESResource, client *ESClient, currentReplicas int32) error {
	as := NewAutoScaler(es, o.metricsInterval, client)

	scalingOperation, err := as.GetScalingOperation()
	if err != nil {
		return err
	}

	name := eds.Name
	namespace := eds.Namespace

	// update EDS definition.
	if scalingOperation.NodeReplicas != nil && *scalingOperation.NodeReplicas != currentReplicas {
		now := metav1.Now()
		if *scalingOperation.NodeReplicas > currentReplicas {
			eds.Status.LastScaleUpStarted = &now
		} else {
			eds.Status.LastScaleDownStarted = &now
		}
		log.Infof("Updating last scaling event in EDS '%s/%s'", namespace, name)

		// update status
		eds, err = o.kube.ZalandoV1().ElasticsearchDataSets(eds.Namespace).UpdateStatus(ctx, eds, metav1.UpdateOptions{})
		if err != nil {
			return err
		}
		eds.Spec.Replicas = scalingOperation.NodeReplicas
	}

	// TODO: move to a function
	jsonBytes, err := json.Marshal(scalingOperation)
	if err != nil {
		return err
	}

	if scalingOperation.ScalingDirection != NONE {
		eds.Annotations[esScalingOperationKey] = string(jsonBytes)

		// persist changes of EDS
		log.Infof("Updating desired scaling for EDS '%s/%s'. New desired replicas: %d. %s", namespace, name, *eds.Spec.Replicas, scalingOperation.Description)
		_, err = o.kube.ZalandoV1().ElasticsearchDataSets(eds.Namespace).Update(ctx, eds, metav1.UpdateOptions{})
		if err != nil {
			return err
		}
	}
	return nil
}

func getOwnerUID(objectMeta metav1.ObjectMeta) (types.UID, bool) {
	if len(objectMeta.OwnerReferences) == 1 {
		return objectMeta.OwnerReferences[0].UID, true
	}
	return "", false
}

// templateInjectLabels injects labels into a pod template spec.
func templateInjectLabels(template v1.PodTemplateSpec, labels map[string]string) v1.PodTemplateSpec {
	if template.ObjectMeta.Labels == nil {
		template.ObjectMeta.Labels = map[string]string{}
	}

	for key, value := range labels {
		template.ObjectMeta.Labels[key] = value
	}
	return template
}

// edsScalingOperation returns the scaling operation read from the
// scaling-operation annotation on the EDS. If no operation is defined it will
// return nil.
func edsScalingOperation(eds *zv1.ElasticsearchDataSet) (*ScalingOperation, error) {
	if op, ok := eds.Annotations[esScalingOperationKey]; ok {
		scalingOperation := &ScalingOperation{}
		err := json.Unmarshal([]byte(op), scalingOperation)
		if err != nil {
			return nil, err
		}
		return scalingOperation, nil
	}
	return nil, nil
}

// validateScalingSettings checks that the scaling settings are valid.
//
//   - min values can not be > than the corresponding max values.
//   - min can not be less than 0.
//   - 'replicas', 'indexReplicas' and 'shardsPerNode' settings should not
//     conflict.
func validateScalingSettings(scaling *zv1.ElasticsearchDataSetScaling) error {
	// don't validate if scaling is not enabled
	if scaling == nil || !scaling.Enabled {
		return nil
	}

	// Prevent scale-to-zero: minReplicas must be at least 1 when autoscaling is enabled
	if scaling.MinReplicas < 1 {
		return fmt.Errorf(
			"minReplicas must be at least 1 when autoscaling is enabled (got %d)",
			scaling.MinReplicas,
		)
	}

	// check that min is not greater than max values
	if scaling.MinReplicas > scaling.MaxReplicas {
		return fmt.Errorf(
			"minReplicas(%d) can't be greater than maxReplicas(%d)",
			scaling.MinReplicas,
			scaling.MaxReplicas,
		)
	}

	if scaling.MinIndexReplicas > scaling.MaxIndexReplicas {
		return fmt.Errorf(
			"minIndexReplicas(%d) can't be greater than maxIndexReplicas(%d)",
			scaling.MinIndexReplicas,
			scaling.MaxIndexReplicas,
		)
	}

	if scaling.MinShardsPerNode > scaling.MaxShardsPerNode {
		return fmt.Errorf(
			"minShardsPerNode(%d) can't be greater than maxShardsPerNode(%d)",
			scaling.MinShardsPerNode,
			scaling.MaxShardsPerNode,
		)
	}

	// ensure that relation between replicas and indexReplicas is valid
	if scaling.MinReplicas < (scaling.MinIndexReplicas + 1) {
		return fmt.Errorf(
			"minReplicas(%d) can not be less than minIndexReplicas(%d)+1",
			scaling.MinReplicas,
			scaling.MinIndexReplicas,
		)
	}

	if scaling.MaxReplicas < (scaling.MaxIndexReplicas + 1) {
		return fmt.Errorf(
			"maxReplicas(%d) can not be less than maxIndexReplicas(%d)+1",
			scaling.MaxReplicas,
			scaling.MaxIndexReplicas,
		)
	}

	return nil
}
